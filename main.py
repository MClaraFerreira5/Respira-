import os
import numpy as np
import sounddevice as sd
import time
from datetime import datetime

from app.services.processing.audio_history import AudioHistory
from app.services.processing.audio_processing import preprocess_audio
from app.services.prediction_recognition import predict_sound, get_class_name, is_event
from app.services.visualization.visualization import generate_spectrogram
from app.services.utils import save_event_audio
from app.services.processing.noise_processing import capture_noise_profile
from app.services.prediction_recognition.speech_recognition import recognize_keyword
from app.core.config import SAMPLE_RATE, RECORD_DURATION, CLASS_INDICES, THRESHOLDS

# Configura o FFmpeg para o PyDub
os.environ["PATH"] += os.pathsep + 'C:\\ffmpeg\\bin'


def record_audio(duration=RECORD_DURATION, sample_rate=SAMPLE_RATE):
    try:
        print(f"Gravando {duration} segundos de √°udio...")
        audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')
        sd.wait()
        return np.squeeze(audio)
    except Exception as e:
        print(f"Erro ao gravar √°udio: {e}")
        return np.zeros(int(duration * sample_rate))


def main():
    print("\n=== Sistema Avan√ßado de Detec√ß√£o de √Åudio ===")
    history = AudioHistory(max_length=10)
    session_start = datetime.now()
    noise_profile = capture_noise_profile()
    event_counts = {label: 0 for label in CLASS_INDICES if label in THRESHOLDS}

    try:
        while True:
            audio = record_audio()
            processed = preprocess_audio(audio)

            if np.max(np.abs(processed)) < 0.01:
                print("Som muito fraco ap√≥s pr√©-processamento. Ignorado.")
                time.sleep(1)
                continue

            _, _, prediction = predict_sound(processed)
            history.add_prediction(prediction, processed)
            smoothed_pred = history.smooth()

            top_class_idx = np.argmax(smoothed_pred)
            top_class_name = get_class_name(top_class_idx)
            confidence = smoothed_pred[top_class_idx]

            print(f"Som predominante: {top_class_name} ({confidence:.2%})")

            generate_spectrogram(processed, smoothed_pred)

            if recognize_keyword(processed, SAMPLE_RATE):
                print("üÜò Palavra-chave 'ajuda' detectada!")
                save_event_audio(processed, SAMPLE_RATE, "ajuda", 1)

            for label in THRESHOLDS:
                if is_event(smoothed_pred, label):
                    event_counts[label] += 1
                    print(
                        f"\nüîç {label.upper()} DETECTADO (Confian√ßa: {smoothed_pred[CLASS_INDICES[label]]:.2%}) Total: {event_counts[label]}")
                    save_event_audio(processed, SAMPLE_RATE, label.lower(), event_counts[label])

            time.sleep(0.5)

    except KeyboardInterrupt:
        print("\nSess√£o encerrada pelo usu√°rio.")
        for label, count in event_counts.items():
            print(f"{label}: {count} eventos detectados")


if __name__ == "__main__":
    main()

# def main():
#     print("\n=== Sistema Avan√ßado de Detec√ß√£o de Tosse ===")
#     print("Pressione Ctrl+C para sair\n")
#
#     history = AudioHistory(max_length=10)
#     #cough_count = 0
#     #sneeze_count = 0
#     #snore_count = 0
#     #gasp_count = 0
#     session_start = datetime.now()
#
#     noise_profile = capture_noise_profile()
#     event_counts = {label: 0 for label in CLASS_INDICES if label in THRESHOLDS}  # CORRIGIDO: fora do while
#
#     try:
#         while True:
#             raw_audio = record_audio()
#             filtered_audio = preprocess_audio(raw_audio)
#             # filtered_audio = remove_static_noise(filtered_audio, noise_profile)  # Desativado para testes
#
#             if np.max(np.abs(filtered_audio)) < 0.01:
#                 print("Som muito fraco ap√≥s filtro. Ignorado.")
#                 time.sleep(1)
#                 continue
#
#             _, _, prediction_recognition = predict_sound(filtered_audio)
#             history.add_prediction(prediction_recognition, filtered_audio)
#             smoothed_pred = history.smooth()
#
#             top_class_idx = np.argmax(smoothed_pred)
#             top_class_name = get_class_name(top_class_idx)
#             confidence = smoothed_pred[top_class_idx]
#             top3_indices = smoothed_pred.argsort()[-3:][::-1]
#             print("Top 3 classes:")
#             for idx in top3_indices:
#                 name = get_class_name(idx)
#                 conf = smoothed_pred[idx]
#             print(f"- {name}: {conf:.2%}")
#             print(f"Som predominante: {top_class_name} ({confidence:.2%})")
#
#             generate_spectrogram(filtered_audio, SAMPLE_RATE, smoothed_pred)  # Mostrar sempre para debug
#
#             if recognize_keyword(filtered_audio, SAMPLE_RATE):
#                 print("üÜò Palavra-chave 'ajuda' detectada!")
#                 save_event_audio(filtered_audio, SAMPLE_RATE, "ajuda", 1)  # Salvando como evento especial
#                 # Voc√™ pode adicionar outras a√ß√µes aqui, como enviar alerta, tocar som, etc.
#
# for label, idx in CLASS_INDICES.items(): if label in THRESHOLDS and smoothed_pred[idx] > THRESHOLDS[label]:
# event_counts[label] += 1 print( f"\nüîç {label.upper()} DETECTADO (Confian√ßa: {smoothed_pred[idx]:.2%}) [Total: {
# event_counts[label]}]") save_event_audio(filtered_audio, SAMPLE_RATE, label.lower(), event_counts[label])
#
#             time.sleep(0.5)
#
#     except KeyboardInterrupt:
#         print("\nSess√£o encerrada pelo usu√°rio.")
#         for label, count in event_counts.items():
#             print(f"{label}: {count} eventos detectados")

# # Configura√ß√µes
# MODEL_URL = 'https://tfhub.dev/google/yamnet/1'
# SAMPLE_RATE = 16000  # YAMNet requer √°udio em 16kHz
# RECORD_DURATION = 3  # Dura√ß√£o da grava√ß√£o em segundos
# THRESHOLDS = {        # Modifica√ß√£o: thresholds espec√≠ficos por classe
#     'Cough': 0.15,
#     'Snore': 0.15,
#     'Breathing': 0.15,
#     'Sneeze': 0.15,
#     'Gasp': 0.15,
#     'Speech': 0.15
# }
#
# MIN_COUGH_DURATION = 0.2  # Dura√ß√£o m√≠nima de uma tosse em segundos
# COUGH_PATTERN_COUNT = 3  # N√∫mero m√≠nimo de tosses para caracterizar um padr√£o
# KEYWORD = "ajuda"
#
# # Carrega o modelo YAMNet
# model = hub.load(MODEL_URL)
#
# # √çndices das classes no YAMNet
# CLASS_INDICES = {
#     'Cough': 42,
#     'Snore': 38,
#     'Breathing': 36,
#     'Speech': 65,
#     'Silence': 1,
#     'Sneeze': 44,
#     'Sniff': 45,
#     'Gasp': 39
# }

# class AudioHistory:
#     def __init__(self, max_length=10):
#         self.max_length = max_length
#         self.predictions = []
#         self.audio_segments = []
#         self.timestamps = []
#
#     def add_prediction(self, prediction_recognition, audio_segment=None):
#         self.predictions.append(prediction_recognition)
#         self.audio_segments.append(audio_segment)
#         self.timestamps.append(datetime.now())
#
#         if len(self.predictions) > self.max_length:
#             self.predictions.pop(0)
#             self.audio_segments.pop(0)
#             self.timestamps.pop(0)
#
#     def smooth(self):     # Modifica√ß√£o: suaviza√ß√£o de predi√ß√µes
#         if not self.predictions:
#             return np.zeros(521)
#         return np.mean(self.predictions, axis=0)
#
#     def get_recent_pattern(self, window_size=3):
#         if len(self.predictions) >= window_size:
#             return self.predictions[-window_size:]
#         return None
#
#     def get_cough_count(self, threshold=0.3):
#         return sum(1 for p in self.predictions
#                    if p[CLASS_INDICES['Cough']] > threshold)

# def capture_noise_profile(duration=2):    # Modifica√ß√£o: captura de ru√≠do ambiente
#     print("Capturando perfil de ru√≠do de fundo...")
#     audio = sd.rec(int(duration * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='float32')
#     sd.wait()
#     return np.squeeze(audio)
#
# def remove_static_noise(audio, noise_profile):
#     energy = lambda x: np.sqrt(np.mean(x**2))
#     if energy(noise_profile) == 0:
#         return audio
#     noise_energy = energy(noise_profile)
#     audio_energy = energy(audio)
#     if audio_energy < 1.5 * noise_energy:
#         return np.zeros_like(audio)
#     return audio

# def preprocess_audio(audio):
#     audio = audio / (np.max(np.abs(audio)) + 1e-8)
#     lowcut = 50
#     highcut = 3000
#     nyquist = 0.5 * SAMPLE_RATE
#     low = lowcut / nyquist
#     high = highcut / nyquist
#     b, a = signal.butter(5, [low, high], btype='band')
#     audio = signal.filtfilt(b, a, audio)
#
#     energy = np.sqrt(np.mean(audio ** 2))
#     threshold = energy * 0.05
#     valid_samples = np.where(np.abs(audio) > threshold)[0]
#
#     if len(valid_samples) > 0:
#         start = max(0, valid_samples[0] - 100)
#         end = min(len(audio), valid_samples[-1] + 100)
#         audio = audio[start:end]
#
#     return audio.astype(np.float32)


# def generate_spectrogram(audio, sample_rate=SAMPLE_RATE, prediction_recognition=None):
#     plt.figure(figsize=(12, 8))
#
#     plt.subplot(3, 1, 1)
#     spec, freqs, times, im = plt.specgram(audio, Fs=sample_rate,
#                                           cmap=cm.viridis, NFFT=512,
#                                           noverlap=256)
#     plt.colorbar(format='%+2.0f dB')
#     plt.title('Espectrograma do √Åudio')
#     plt.xlabel('Tempo (s)')
#     plt.ylabel('Frequ√™ncia (Hz)')
#
#     plt.subplot(3, 1, 2)
#     time_axis = np.linspace(0, len(audio) / sample_rate, num=len(audio))
#     plt.plot(time_axis, audio)
#     plt.title('Forma de Onda')
#     plt.xlabel('Tempo (s)')
#     plt.ylabel('Amplitude')
#     plt.grid(True)
#
#     if prediction_recognition is not None:
#         plt.subplot(3, 1, 3)
#         classes = list(CLASS_INDICES.keys())
#         values = [prediction_recognition[idx] for idx in CLASS_INDICES.values()]
#         colors = ['red' if cls == 'Cough' else 'blue' for cls in classes]
#         bars = plt.bar(classes, values, color=colors)
#         plt.title('Probabilidades das Classes')
#         plt.ylabel('Probabilidade')
#         plt.xticks(rotation=45)
#         plt.ylim(0, 1)
#
#         for bar in bars:
#             height = bar.get_height()
#             plt.text(bar.get_x() + bar.get_width() / 2., height,
#                      f'{height:.2f}',
#                      ha='center', va='bottom')
#
#     plt.tight_layout()
#     plt.show()

# def predict_sound(audio):
#     try:
#         waveform = audio / (np.max(np.abs(audio)) + 1e-8)
#         waveform = waveform.astype(np.float32)
#         scores, embeddings, spectrogram = model(waveform)
#         prediction_recognition = np.mean(scores, axis=0)
#         top_class = np.argmax(prediction_recognition)
#         confidence = prediction_recognition[top_class]
#         return top_class, confidence, prediction_recognition
#     except Exception as e:
#         print(f"Erro na predi√ß√£o: {e}")
#         return -1, 0, np.zeros(521)
#
# def is_cough(prediction_recognition, history=None):
#     cough_score = prediction_recognition[CLASS_INDICES['Cough']]
#     silence_score = prediction_recognition[CLASS_INDICES['Silence']]
#     above_threshold = cough_score > THRESHOLDS['Cough']
#     not_silence = silence_score < 0.1
#     is_pattern = False
#     if history is not None and len(history.predictions) >= COUGH_PATTERN_COUNT:
#         recent_coughs = sum(1 for p in history.predictions[-COUGH_PATTERN_COUNT:]
#                             if p[CLASS_INDICES['Cough']] > THRESHOLDS['Cough'])  # Fixed: Use specific threshold
#         is_pattern = recent_coughs >= COUGH_PATTERN_COUNT - 1
#     return above_threshold and not_silence and (is_pattern or cough_score > 0.5)

# def is_sneeze(prediction_recognition):
#     return prediction_recognition[CLASS_INDICES['Sneeze']] > THRESHOLDS['Sneeze']  # Fixed: Use specific threshold
#
# def is_snore(prediction_recognition):
#     return prediction_recognition[CLASS_INDICES['Snore']] > THRESHOLDS['Snore']  # Fixed: Use specific threshold
#
# def is_gasp(prediction_recognition):
#     return prediction_recognition[CLASS_INDICES['Gasp']] > THRESHOLDS['Gasp']  # Fixed: Use specific threshold

# def recognize_keyword(audio, sample_rate):
#     """Realiza reconhecimento de fala e verifica se palavra-chave est√° presente"""
#     try:
#         filename = "temp_speech.wav"
#         sf.write(filename, audio, sample_rate)
#         recognizer = sr.Recognizer()
#         with sr.AudioFile(filename) as source:
#             audio_data = recognizer.record(source)
#         transcript = recognizer.recognize_google(audio_data, language="pt-BR")
#         print(f"Transcri√ß√£o: {transcript}")
#         return KEYWORD.lower() in transcript.lower()
#     except Exception as e:
#         print(f"Erro no reconhecimento de fala: {e}")
#         return False

# def save_event_audio(audio, sample_rate, label, count):
#     os.makedirs(f"{label}_detections", exist_ok=True)
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"{label}_detections/{label}_{timestamp}_{count}.wav"
#     sf.write(filename, audio, sample_rate)
#     print(f"√Åudio de {label} salvo como {filename}")


# def get_class_name(class_index):
#     for name, idx in CLASS_INDICES.items():
#         if idx == class_index:
#             return name
#     return 'Other'
